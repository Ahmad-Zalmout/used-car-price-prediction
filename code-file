# Used cars price prediction
### Authored by Ahmad Zalmout

## Abstract

In this document, a dataset of used cars was used to predict the selling price of the car based on a selection of features, for instance, number of seats, distance travelled, engine size, and so on. <br>
The dataset was collected from [Used Car Price Dataset](https://www.kaggle.com/datasets/rishabhkarn/used-car-dataset). The collection of the dataset is not clearly stated however, after some heavy research it was concluded that the data might be collected from an Indian cars selling [website](https://www.cardekho.com/). <br>

The reasons being the following:
1. The presense of a column "Price(in lakhs)" and that is an indian numbering system used to refer to large amounts and is usually related to the currency Rupees.
2. Everytime a car in the dataset is searched online, it is found in this website mentioned having almost all the features present in the dataset.
<br>

In the document, regression models were used for predicting the price (in euro) after some heavy data cleaning.
## Introduction

This document is intended as a project for a Data Science student to practice some data exploration techniques and the use of limited knowledge in ML models to try to predict prices of used cars as acurate as possible.<br>
In this exercise, a number of ML regression models were used to examine which would produce better results than the other.<br>
The models were exposed to tuning in order to achieve the required results.
## Data Preprocessing

To work with ML models, a few steps are needed to prepare the data for prediction.
The techniques used in this project are the following:

1. Removing Redundant columns.
2. Renaming misleading column name.
3. Checking and removing "NA" values.
4. Checking and removing duplicate values.
5. Checking data consistency across each column.
    1. Making sure categorical columns contain only categories.
    2. Making sure numerical columns contain reasonable ranges of values.
6. Exploratory data analysis.
# Importing needed libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score
import category_encoders as ce
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures
import plotly.offline as pyo
pyo.init_notebook_mode(connected=True)
# Reading the CSV file into a dataframe

df = pd.read_csv("Used Car Dataset.csv")
df
#### Removing Redundant columns and renaming misleading column name.
# Removing unnamed column as dataframe already has an index column
df.drop(columns = 'Unnamed: 0', inplace = True)

# removing column max_power(bhp) as it is a duplicate from column engine(cc)
df.drop(columns = 'max_power(bhp)', axis = 1, inplace = True)

# Renaming column
df = df.rename(columns={'ownsership': 'ownership'})
# Showing information about the data
df.info()
#### Checking and removing "NA" values.
It seams that there are missing data in the columns: mileage(kmpl), engine(cc), max_power(bhp), torque(Nm)
# Calculating sum of na values if any
df['mileage(kmpl)'].isna().sum()
# Removing na values
df.dropna(inplace = True)
# making sure na values are removed from all columns
df.info()
#### Checking and removing duplicate values.
# Checking for duplicates
df.duplicated().sum()
# Removing duplicates
df = df.loc[~df.duplicated()].reset_index(drop = True)
# Checking number of rows in each column after removing duplicates
df.info()
#### Checking data consistency across each column.
    - Making sure categorical columns contain only categories.
    - Making sure numerical columns contain reasonable ranges of values.
# Displaying uniques values with thier count in column ownership.
df['ownership'].value_counts()
It apears to be there are misleading values. It might be misplacement error, but since they are low in number they will be removed.
# Removing misleading values.
garbage_data = df[df['ownership'].str.contains('^(1|2|9)')].index.to_list()
df.drop(garbage_data, inplace = True)
df.reset_index(drop = True, inplace = True)
# Making sure changes took place.
df['ownership'].value_counts()
# Displaying unique values with thier count in  column transmission.
df['transmission'].value_counts()
This column looks clean
# Displaying unique values with their count in  column manufacturing year.
df['manufacturing_year'].value_counts()
This column is clean
# Displaying unique values with their count in column seats.
df['seats'].value_counts()
This column is clean
# Displaying unique values with their count in column fuel type.
df['fuel_type'].value_counts()
This column is clean
# Displaying unique values with thier count in column insurance validity.
df['insurance_validity'].value_counts()
# Fixing labeling error
df['insurance_validity'][df['insurance_validity'] == "Third Party insurance"] = "Third Party"

# Removing "Not available" row
df.drop(index = df[df['insurance_validity'] == 'Not Available'].index, inplace = True)
df.reset_index(drop = True, inplace = True)
df.info()
Data set looks good so far
df.describe()
As shown above, there are a lot of values that are out of context, for instance, the max value of the column engine is 3.258640e+12, aLso, torque values are extremely high.

When it comes to the price, it might be confusing for some as it is in lakhs, which is in indian numbering system which translates to 100,000. And as mentioned earlier usually this is associated with the currency Rupees. On the other hand, a car cannot possibly cost 9,500,000,000 Rupees or 104,569,248.82 Euros! Especially after inspecting it which turned out to be a really humble car.

In addition, the mean in all the above columns except of "seats" is much higher than the median. Suggesting positive sknewness. 
# Displaying the values in the data set sorted accourding to the price
df[['car_name', 'price(in lakhs)']].sort_values(by = 'price(in lakhs)', ascending = False)
The "price(in lakhs)" column have extremely large values. But it turns out that only 2 entries are misleading, and only needed lowering the number of extra zeros. 

This was double checked from the [website](https://www.cardekho.com/) the data is taken from.
# Reducing the number of zeros for the 2 entries as they are recorded incorrectly.
df['price(in lakhs)'] = df['price(in lakhs)'].apply(lambda x: x/10000 if x > 1000 else x)
# Displaying the data set sorted accourding to the engine and mileage columns
df[['car_name', 'mileage(kmpl)', 'engine(cc)']].sort_values(by = ['engine(cc)', 'mileage(kmpl)'], ascending = False)
The values in columns "engine(cc)" and "mileage(kmpl)" appear to be swapped. As it is impossible for a car to have mileage of 2925 and an engine capacity of 3.258640e+12. So the values of the engine column were double checked from the [website](https://www.cardekho.com/).

However, the actual values of the mileage coud not be obtained and it is dificult to reduce the digit count as it is unique to each car based on its usage.
# Swapping values between columns.
mask = df[df['mileage(kmpl)'] > 100].index
temp_values = df.loc[mask, ['mileage(kmpl)', 'engine(cc)']].copy()
df.loc[mask, 'engine(cc)'] = temp_values['mileage(kmpl)']
df.drop(columns = 'mileage(kmpl)', inplace = True)
# Displaying the values of torque that are inconsistant
df[['car_name', 'torque(Nm)']][df['torque(Nm)'] / 1000 > 1].sort_values(by = 'torque(Nm)', ascending = False)
As you can see, these values are too large for torque. However after heavy inspection it was **not** found what those values represent. <br>
In addition, droping the values would result in a large lose of data. <br>
For this reason, it was a good choice to drop the column entirely.
# Droping column torque
df.drop(columns = 'torque(Nm)', inplace = True)
# Displaying values of registration year column.
df['registration_year'].sort_values()
As shown above, the column does not have consistent values. 
The month cannot be added for the rows that don't have them as this information was not found. 
Also, removing the months would result in redundant values as there exist a column with manufacturing year.
# Droping registration year coulmn.
df.drop(columns = 'registration_year', inplace = True)
The car prices are in Rupees Lakhs. As mentioned earlier, this translates to 100,000 Rupees. <br>
Thecode cell below converts the prices to Euro.
# Converting lakhs to euro
df['price(euro)'] = df['price(in lakhs)'] * 1100.73
#### Exploratory data analysis

A number of graphs were ploted to see the distribution of the data to understand and explain the results of the model.
# Ploting price of cars against engine size
plt.scatter(df['engine(cc)'] / 1000, df['price(euro)'] / 1000, s = 20)
plt.style.use('seaborn-v0_8-dark')
plt.title("Price verses engine size")
plt.xlabel("Engine size (1000 cc)")
plt.ylabel('Price (1000 euro)')
The graph above shows that the distribution of engine size is concentrated in the range 1000 - 3000 cc. <br>
Price is spread on the whole spectrum.

As a general point of view, the larger the engine size, the higher the price gets. However, up to a 3000 cc some prices drop to the extremely low range. This seemed counter intuitive. <br>
As it turns out, the cars that are above 3000 and some times 2000 cc with prices in lakhs in the single digit value they are actually in crore and not lakh. This means the price is actually in 10,000,000 Rupees instead of 100,000. So after the conversion they turned out to be extrmely cheap while they belong to the luxurious category of vehicles.

All that being said, the problem cannot be solved easily as the not only the engine size and the price can determine this car is in lakh or crore, but also the manufacturer of the vehicle and some common sense. And this in python is not easily solved.
# Ploting manufacturing year.
sns.boxplot(df['manufacturing_year'])
sns.set_theme(style="whitegrid")
plt.ylabel("Year of manufacturing")
plt.title("Box Plot of Manufacturing year")
The box plot above displays the manufacturing year of the vehicles. <br>
It is noted that most of the cars are new as the data is positivly skewed in terms of manufacturing year. <br>
This might make the prediction to be closer to the price of new cars which in turn over estimate the price.
# Ploting distance driven.
sns.boxplot(df['kms_driven'])
plt.ylabel("Distance driven(Km)")
plt.title("Box Plot of distance driven")
# Ploting the engine size.
sns.boxplot(df['engine(cc)'])
plt.ylabel("Engine Capacity (cc)")
plt.title("Box Plot of engine capacity")
# Ploting prices.
sns.boxplot(df['price(euro)'])
plt.ylabel("Vehicle price (euro)")
plt.title("Box Plot of vehicle price")
In the above 3 plot above, it is noted that the data a lot of outliers. <br>
This have to be dealt with because it will affect the prediction.
# Removing outliers
min_threshold1, max_threshold1 = df['kms_driven'].quantile([0.001,0.999])
min_threshold2, max_threshold2 = df['engine(cc)'].quantile([0.001,0.999])
min_threshold3, max_threshold3 = df['price(euro)'].quantile([0.001,0.999])

df = df[(df['kms_driven']<max_threshold1)&(df['kms_driven']>=min_threshold1)]
df = df[(df['engine(cc)']<max_threshold2)&(df['engine(cc)']>=min_threshold2)]
df = df[(df['price(euro)']<max_threshold3)&(df['price(euro)']>=min_threshold3)]
df.reset_index(drop=True,inplace=True)
# Ploting the fuel type
plt.style.use('seaborn-v0_8')
plt.bar(df['fuel_type'].unique(), df['fuel_type'].value_counts(), color = 'green')
# plt.grid(axis='y', linestyle='--', alpha=0.9)
plt.title('Fuel types count')
plt.xlabel('Fuel Types')
plt.ylabel('Vehicle count')
plt.show()
It is shown in the plot above that most of the data is _Petrol_. This might be a problem for later as the prediction will be biased to Petrol cars. <br> 
And equating the data accourding to fuel type is not possible as the CNG has very few entries. <br><br>
Note: CNG fuel is Compressed Natural Gas.
# Ploting number of seats.
plt.bar(df['seats'].unique(), df['seats'].value_counts(), color = 'orange')
plt.title('Seats count')
plt.xlabel('Number of seats')
plt.ylabel('Vehicle count')
plt.show()
In terms of seats, the majority of the data is 5 seats. This will cause a bias toward the 5 seater vehicles. 
# Ploting Ownership categories.
fig = go.Figure(data=[go.Pie(labels=df['ownership'].unique(), values=df['ownership'].value_counts())])

fig.update_layout(title_text='Ownership categories',
                  width=500,
                  height=400)
pyo.iplot(fig)
The majority of the ownership is first owner. And the fifth owner is barely visible. Resulting yet in another bias.
## Machine learning Models

Below is 3 regression models that are applied on the dataset for the purpose of predicting the price of a car based no the features explored earlier. <br>
The models are predicting the price in Euro.
# Preparing X and Y values
X = df.drop(columns=['price(in lakhs)', 'price(euro)', 'car_name'])
Y = df['price(euro)']
# Encoding categorical features using onehot encoder
encoder = ce.OneHotEncoder(cols=['insurance_validity', 'fuel_type', 'ownership', 'transmission'])
X_transformed = encoder.fit_transform(X)

# Standardizing the data
scaler = StandardScaler()
X_transformed = scaler.fit_transform(X_transformed)
X_transformed = pd.DataFrame(X_transformed)

# selecting best featues that describe the data
k=8
selector = SelectKBest(f_regression,k=k)
selector.fit(X_transformed, Y)
best_feats = selector.get_support(indices=True)
xTrain_best = X_transformed.iloc[:,best_feats]
# Splitting the data 
X_train, X_test, y_train, y_test = train_test_split(xTrain_best, Y, test_size=0.2, random_state=5)
# Applying a linear regression model

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Printing the actual and predicted values
print("Actual value", "         Predicted value\n")
i = 1
while i < 10:
    print(y_test.values[i],"            ", y_pred[i])  
    i += 1
print("-" *50)
print("Evaluation metrics: ")


mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
The MSE is extremely high. <br>
The R squared shows the prediction is weakly represented. <br>
And as expected, most of the predicted values are over estimated as stated above in the graph of manufacturing year.<br>
This means this model is not good. However, more than one option of choosing best features were chosen, outliers removed, data cleaned, and a number of variations of encoding was applied on the data. And still there were no significant imporovment.

This may lead to the conclusion that this model cannot be represented in linear model. A possible cause is the high number of categorical data.
# Applying a polynomial regression model
degrees = [1, 5, 10]
for i in degrees:
    poly_reg2=PolynomialFeatures(degree=i)

    # transforming feature X into X^2
    X_poly=poly_reg2.fit_transform(X_train)

    # creating a linear regression handler
    lin_reg_2=LinearRegression()

    # applying linear regression to the transformed X^2 space
    # this is equivalent to fitting a polynomial of degree 2 to the orginal X space
    lin_reg_2.fit(X_poly,y_train)
    y_pred2 = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred2)
    r2 = r2_score(y_test, y_pred2)
    print(f"Polynomial degree {i}:\n")

    print(f'Mean Squared Error: {mse}')
    print(f'R-squared: {r2}')
    print("-" * 20)
Same exact numbers are obtained using polynomial regression. <br>
This might indicate that the data cannot be represented in higher degrees of polynomial regression.

A different model is applied below in an attempt to predict the prices more accurately.
import numpy as np
from sklearn.svm import SVR
import matplotlib.pyplot as plt


from sklearn.model_selection import GridSearchCV

param_grid = {'C': [1, 10, 100, 1000],
              'gamma': ['auto', 0.1, 0.01, 0.001],
              'epsilon': [0.1, 0.5, 1, 2]}

grid_search = GridSearchCV(SVR(kernel='rbf'), param_grid, cv=5)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print(f"Best Parameters: {best_params}")

best_svr = SVR(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], epsilon=best_params['epsilon'])
best_svr.fit(X_train, y_train)
y_pred3 = best_svr.predict(X_test)


mse = mean_squared_error(y_test, y_pred3)
print(f"Mean Squared Error (MSE): {mse}")

r2 = r2_score(y_test, y_pred3)
print(f"R-squared (R2): {r2}")
The SVR (Support Vector Regression) with RBF kernel is used with different parameters. <br>
A grid search method was used to find the best parameters for the model and the same MSE and r squared were obtained. <br>
This indicates that even this algorithm cannot represent the data nor predict the price.

It is worth mentioning that due to this algorithm, the standard encoding was added. As it was adviced to standardise the data before a SVM (support Vector Machine) model is used for optimum results. And infact it did help, it raised the r sqaured from -0.16 to 0.37. However it remained a bad model for this data and cannot be counted on for price prediction accoring to this dataset.
## Conclusion

Despite the fact that 3 ML models were used on different parameters and degrees, it was not possible to come up with a model that predicts the price based on this dataset. because the MSE is very large and the r squared does not represent a strong relationship between the test set and the predicted set. <br>
This might be due to the following reasons:

1. Limited number of ML models were used.
    
    Although the dataset appears to be relatively large and having a large number of features, it was not suffiecent for the 3 regression models used above to make an acurate prediction. Perhaps if different models were used like KNN (K-Nearest Number) or Decision Tree were used, they might give better results.

2. Not enough Data Cleaning.

    Another possible senario is limited data cleaning techniques. This might be due to the limited time of the project to be finished or due to the limited experience of the implementer of the data cleaning techniques. <br>
    Despite that, substantial effort was put into cleaning the data as mentioned above in detail. The data is so bad that even the auther himself has questioned his dataset  and felt confused after being asked about the problems his data have. Considerable number of data analysts online critisized the dataset and even some of them tried cleaning it only to post the cleaned version. <br>

    As mentioned earlier, the dataset had so many problem including: 

    1. Swapped values.
    2. values recorded with much more digits and zeros than actual values.
    3. redundant columns.
    4. price unit is not accurate.
    5. filled with outliers.

    After all, those problems were solved before implementing any model and it was made sure the data is clean both statistically and logically. On the other hard, some problems were discovered at a later stage, for instance, the incorret price unit that was *crore* instead of *lakh*, this alone can cause severe problems as the car would be predicted to have high price value according to its features but when compared to the test set it would result in a big difference and thus cause high error value.

3. Categorical Features
    One possible cause is the presence of large number of categorical features especially after data cleaning. This resutls in a high number of columns being encoded and transformed and thus not giving a good representation on the data.

    
At the end, it is highly likely that the dataset is the main responsible of the high errors the models produced and the problem was not the tuning nor the data cleaning process. However, other possible explainations are possible.
